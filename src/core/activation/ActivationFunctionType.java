/********************************************************
 * SANNet Neural Network Framework
 * Copyright (C) 2018 - 2019 Simo Aaltonen
 *
 ********************************************************/

package core.activation;

/**
 * Defines supported activation function.<br>
 * Following functions are supported:
 *     LINEAR,
 *     SIGMOID,
 *     HARDSIGMOID,
 *     BIPOLARSIGMOID,
 *     TANH,
 *     TANSIG,
 *     TANHAPPR,
 *     HARDTANH,
 *     EXPONENTIAL,
 *     SOFTPLUS,
 *     SOFTSIGN,
 *     RELU,
 *     ELU,
 *     SELU,
 *     GELU,
 *     SOFTMAX,
 *     GAUSSIAN,
 *     SIN
 *
 */
public enum ActivationFunctionType {
    LINEAR,
    SIGMOID,
    HARDSIGMOID,
    BIPOLARSIGMOID,
    TANH,
    TANSIG,
    TANHAPPR,
    HARDTANH,
    EXPONENTIAL,
    SOFTPLUS,
    SOFTSIGN,
    RELU,
    ELU,
    SELU,
    GELU,
    SOFTMAX,
    GAUSSIAN,
    SIN
}
